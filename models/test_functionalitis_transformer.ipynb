{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 50, 10])\n",
      "torch.Size([5, 50])\n",
      "torch.Size([5, 10])\n",
      "tensor([[-0.5688, -0.3338, -0.1661, -0.0249, -0.0462, -0.4219,  0.1485,  0.0539,\n",
      "          0.3579,  0.0553],\n",
      "        [-0.0784,  0.2849, -0.1677,  0.2220, -0.1155,  0.1715,  0.1597,  0.2521,\n",
      "          0.0316,  0.0373],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "\n",
    "def masked_mean_pooling(datatensor, mask):\n",
    "    \"\"\"\n",
    "    Computes the masked mean pooling of the input tensor along the specified dimension.\n",
    "\n",
    "    This function calculates the average of the values in the input tensor `datatensor` \n",
    "    while ignoring the values at positions where the `mask` tensor is zero. It is \n",
    "    particularly useful for handling sequences of varying lengths where padding is \n",
    "    applied.\n",
    "\n",
    "    Args:\n",
    "        datatensor (torch.Tensor): The input tensor of shape (batch_size, lenght_time, num_features).\n",
    "        mask (torch.Tensor): A binary mask tensor of shape (batch_size, sequence_length) \n",
    "                             where 1 indicates valid data points and 0 indicates padding.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (batch_size, feature_dim) containing the masked \n",
    "                      mean pooled values for each sequence in the batch.\n",
    "    \"\"\"\n",
    " \n",
    " \n",
    "    # eliminate all values learned from nonexistant timepoints\n",
    "    mask_expanded = mask.unsqueeze(-1).expand(datatensor.size()).float() # Takes the mask tensor, adds an extra dimension at the end,\n",
    "    # expands it to match the size of datatensor, and converts it to a floating-point tensor.\n",
    "    data_summed = torch.sum(datatensor * mask_expanded, dim=1)\n",
    "\n",
    "    # find out number of existing timepoints\n",
    "    data_counts = mask_expanded.sum(1)\n",
    "    data_counts = torch.clamp(data_counts, min=1e-9)  # put on min clamp\n",
    "\n",
    "    # Calculate average:\n",
    "    averaged = data_summed / (data_counts)\n",
    "\n",
    "    return averaged\n",
    "\n",
    "batch_size = 5\n",
    "feature_dim = 10\n",
    "length_time = 50\n",
    "\n",
    "datatensor = torch.randn(batch_size, length_time, feature_dim)\n",
    "\n",
    "print(datatensor.size())\n",
    "\n",
    "mask = torch.zeros(batch_size, length_time)\n",
    "\n",
    "mask[0, :10] = 1\n",
    "mask[1, :20] = 1\n",
    "\n",
    "print(mask.size())\n",
    "\n",
    "print(masked_mean_pooling(datatensor, mask).size())\n",
    "print(masked_mean_pooling(datatensor, mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500])\n",
      "torch.Size([1, 500, 32])\n",
      "tensor([-0.8218,  0.8080,  0.2857, -0.0298, -0.7708, -0.9994,  0.8192, -0.9973,\n",
      "        -0.0212,  0.8682,  0.9821,  0.7911,  0.5671,  0.3880,  0.2603,  0.1731,\n",
      "         0.5698,  0.5892,  0.9583,  0.9996, -0.6371, -0.0343,  0.5735,  0.0738,\n",
      "        -0.9998, -0.4962,  0.1886,  0.6117,  0.8236,  0.9216,  0.9655,  0.9849])\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncodingTF(nn.Module):\n",
    "    \"\"\"\n",
    "    Based on the SEFT positional encoding implementation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncodingTF, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self._num_timescales = d_model // 2\n",
    "\n",
    "    def getPE(self, P_time):\n",
    "        B = P_time.shape[1]\n",
    "\n",
    "        P_time = P_time.float()\n",
    "\n",
    "        # create a timescale of all times from 0-1\n",
    "        timescales = self.max_len ** np.linspace(0, 1, self._num_timescales)\n",
    "\n",
    "        # make a tensor to hold the time embeddings\n",
    "        times = torch.Tensor(P_time.cpu()).unsqueeze(2)\n",
    "\n",
    "        # scale the timepoints according to the 0-1 scale\n",
    "        scaled_time = times / torch.Tensor(timescales[None, None, :])\n",
    "        # Use a 32-D embedding to represent a single time point\n",
    "        pe = torch.cat(\n",
    "            [torch.sin(scaled_time), torch.cos(scaled_time)], axis=-1\n",
    "        )  # T x B x d_model\n",
    "        pe = pe.type(torch.FloatTensor)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, P_time):\n",
    "        pe = self.getPE(P_time)\n",
    "        return pe\n",
    "    \n",
    "time = torch.randint(0, 500, (1, 500))\n",
    "print(time.size())\n",
    "\n",
    "pos_layer = PositionalEncodingTF(32, 500)\n",
    "embeddings = pos_layer(time)\n",
    "print(embeddings.size())\n",
    "\n",
    "print(embeddings[0, 1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n",
      "tensor([[ 0.0454, -0.1720],\n",
      "        [ 0.0908, -0.1668],\n",
      "        [ 0.0839, -0.0448],\n",
      "        [ 0.0705, -0.1464],\n",
      "        [ 0.1508, -0.2261]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from x_transformers import Encoder\n",
    "\n",
    "\n",
    "def masked_max_pooling(datatensor, mask):\n",
    "    \"\"\"\n",
    "    Adapted from HuggingFace's Sentence Transformers:\n",
    "    https://github.com/UKPLab/sentence-transformers/\n",
    "    Calculate masked average for final dimension of tensor\n",
    "    \"\"\"\n",
    "    # eliminate all values learned from nonexistant timepoints\n",
    "    mask_expanded = mask.unsqueeze(-1).expand(datatensor.size()).float()\n",
    "\n",
    "    datatensor[mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "    maxed = torch.max(datatensor, 1)[0]\n",
    "\n",
    "    return maxed\n",
    "\n",
    "class EncoderClassifierRegular(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        device=\"cpu\",\n",
    "        pooling=\"mean\",\n",
    "        num_classes=2,\n",
    "        sensors_count=37,\n",
    "        static_count=8,\n",
    "        layers=1,\n",
    "        heads=1,\n",
    "        dropout=0.2,\n",
    "        attn_dropout=0.2,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pooling = pooling\n",
    "        self.device = device\n",
    "        self.sensors_count = sensors_count\n",
    "        self.static_count = static_count\n",
    "\n",
    "        self.sensor_axis_dim_in = 2 * self.sensors_count\n",
    "\n",
    "        self.sensor_axis_dim = self.sensor_axis_dim_in\n",
    "        if self.sensor_axis_dim % 2 != 0:\n",
    "            self.sensor_axis_dim += 1\n",
    "\n",
    "        self.static_out = self.static_count + 4\n",
    "\n",
    "        self.attn_layers = Encoder(\n",
    "            dim=self.sensor_axis_dim,\n",
    "            depth=layers,\n",
    "            heads=heads,\n",
    "            attn_dropout=attn_dropout,\n",
    "            ff_dropout=dropout,\n",
    "        )\n",
    "\n",
    "        #This embedding is used for the 37 time series of the pysionet\n",
    "        self.sensor_embedding = nn.Linear(self.sensor_axis_dim_in, self.sensor_axis_dim)\n",
    "\n",
    "        #Static is used for the rest of the constant variables, eg. Age.\n",
    "        self.static_embedding = nn.Linear(self.static_count, self.static_out)\n",
    "        self.nonlinear_merger = nn.Linear(\n",
    "            self.sensor_axis_dim + self.static_out,\n",
    "            self.sensor_axis_dim + self.static_out,\n",
    "        )\n",
    "        self.classifier = nn.Linear(\n",
    "            self.sensor_axis_dim + self.static_out, num_classes\n",
    "        )\n",
    "\n",
    "        self.pos_encoder = PositionalEncodingTF(self.sensor_axis_dim)\n",
    "\n",
    "    def forward(self, x, static, time, sensor_mask, **kwargs):\n",
    "\n",
    "        x_time = torch.clone(x)  # (N, T)\n",
    "        x_time = torch.permute(x_time, (0, 2, 1))  # (N, T)\n",
    "        mask = (\n",
    "            torch.count_nonzero(x_time, dim=2)\n",
    "        ) > 0  # mask for sum of all sensors for each person/at each timepoint\n",
    "\n",
    "        # add indication for missing sensor values\n",
    "        x_sensor_mask = torch.clone(sensor_mask)  # (N, F, T)\n",
    "        x_sensor_mask = torch.permute(x_sensor_mask, (0, 2, 1))  # (N, T, F)\n",
    "        x_time = torch.cat([x_time, x_sensor_mask], axis=2)  # (N, T, 2F) #Binary\n",
    "\n",
    "        # make sensor embeddings\n",
    "        x_time = self.sensor_embedding(x_time)  # (N, T)\n",
    "\n",
    "        # add positional encodings\n",
    "        pe = self.pos_encoder(time).to(self.device)  # taken from RAINDROP, (N, T, pe)\n",
    "        x_time = torch.add(x_time, pe)  # (N, T, F) (N, F)\n",
    "\n",
    "        # run  attention\n",
    "        x_time = self.attn_layers(x_time, mask=mask)\n",
    "\n",
    "        if self.pooling == \"mean\":\n",
    "            x_time = masked_mean_pooling(x_time, mask)\n",
    "        elif self.pooling == \"median\":\n",
    "            x_time = torch.median(x_time, dim=1)[0]\n",
    "        elif self.pooling == \"sum\":\n",
    "            x_time = torch.sum(x_time, dim=1)  # sum on time\n",
    "        elif self.pooling == \"max\":\n",
    "            x_time = masked_max_pooling(x_time, mask)\n",
    "\n",
    "        # concatenate poolingated attented tensors\n",
    "        static = self.static_embedding(static)\n",
    "        x_merged = torch.cat((x_time, static), axis=1)\n",
    "\n",
    "        nonlinear_merged = self.nonlinear_merger(x_merged).relu()\n",
    "\n",
    "        # classify!\n",
    "        return self.classifier(nonlinear_merged)\n",
    "    \n",
    "# Create dummy data\n",
    "#x = torch.randn(batch_size, length_time, feature_dim)\n",
    "x = torch.randn(batch_size, feature_dim, length_time)\n",
    "time = torch.randint(0, 500, (1, 50))\n",
    "\n",
    "\n",
    "static = torch.randn(batch_size, 8)\n",
    "    \n",
    "sensor_mask = torch.zeros(batch_size, feature_dim, length_time)\n",
    "\n",
    "mask[0, :10] = 1\n",
    "mask[1, :20] = 1\n",
    "\n",
    "\n",
    "    # Create an instance of the EncoderClassifierRegular\n",
    "model = EncoderClassifierRegular(device=\"cpu\", pooling=\"mean\", num_classes=2, sensors_count=feature_dim, static_count=8)\n",
    "\n",
    "    # Run the model with the dummy data\n",
    "output = model(x, static, time, sensor_mask)\n",
    "print(output.size())\n",
    "print(output)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
