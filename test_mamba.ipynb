{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DUMMY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 5\n",
    "feature_dim = 37\n",
    "length_time = 50\n",
    "\n",
    "data = torch.randn(batch_size, feature_dim, length_time)\n",
    "time = torch.randint(0, 500, (1, length_time))\n",
    "data_1 = data[0]\n",
    "\n",
    "\n",
    "static = torch.randn(batch_size, 8)\n",
    "static_1 = static[0]\n",
    "    \n",
    "sensor_mask = torch.zeros(batch_size, feature_dim, length_time)\n",
    "\n",
    "sensor_mask[0, :10] = 1\n",
    "sensor_mask[1, :20] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENSOR EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded Output Shape: torch.Size([37, 50, 32])\n",
      "Embedded Output: tensor([[[ 3.2309e-01, -1.1614e+00, -7.1861e-01,  ...,  3.6157e-01,\n",
      "          -3.0024e-01, -3.0289e-01],\n",
      "         [ 3.2309e-01, -1.1614e+00, -7.1861e-01,  ...,  3.6157e-01,\n",
      "          -3.0024e-01, -3.0289e-01],\n",
      "         [ 3.2309e-01, -1.1614e+00, -7.1861e-01,  ...,  3.6157e-01,\n",
      "          -3.0024e-01, -3.0289e-01],\n",
      "         ...,\n",
      "         [ 3.2309e-01, -1.1614e+00, -7.1861e-01,  ...,  3.6157e-01,\n",
      "          -3.0024e-01, -3.0289e-01],\n",
      "         [ 3.2309e-01, -1.1614e+00, -7.1861e-01,  ...,  3.6157e-01,\n",
      "          -3.0024e-01, -3.0289e-01],\n",
      "         [ 3.2309e-01, -1.1614e+00, -7.1861e-01,  ...,  3.6157e-01,\n",
      "          -3.0024e-01, -3.0289e-01]],\n",
      "\n",
      "        [[ 2.2563e-01, -4.8985e-01, -5.7224e-02,  ..., -3.8700e-01,\n",
      "           8.5718e-01,  3.2414e-01],\n",
      "         [ 2.2563e-01, -4.8985e-01, -5.7224e-02,  ..., -3.8700e-01,\n",
      "           8.5718e-01,  3.2414e-01],\n",
      "         [ 2.2563e-01, -4.8985e-01, -5.7224e-02,  ..., -3.8700e-01,\n",
      "           8.5718e-01,  3.2414e-01],\n",
      "         ...,\n",
      "         [ 2.2563e-01, -4.8985e-01, -5.7224e-02,  ..., -3.8700e-01,\n",
      "           8.5718e-01,  3.2414e-01],\n",
      "         [ 2.2563e-01, -4.8985e-01, -5.7224e-02,  ..., -3.8700e-01,\n",
      "           8.5718e-01,  3.2414e-01],\n",
      "         [ 2.2563e-01, -4.8985e-01, -5.7224e-02,  ..., -3.8700e-01,\n",
      "           8.5718e-01,  3.2414e-01]],\n",
      "\n",
      "        [[-2.6868e-03,  4.2130e-01,  6.8803e-01,  ...,  4.6363e-01,\n",
      "           6.4152e-01,  3.0779e-01],\n",
      "         [-2.6868e-03,  4.2130e-01,  6.8803e-01,  ...,  4.6363e-01,\n",
      "           6.4152e-01,  3.0779e-01],\n",
      "         [-2.6868e-03,  4.2130e-01,  6.8803e-01,  ...,  4.6363e-01,\n",
      "           6.4152e-01,  3.0779e-01],\n",
      "         ...,\n",
      "         [-2.6868e-03,  4.2130e-01,  6.8803e-01,  ...,  4.6363e-01,\n",
      "           6.4152e-01,  3.0779e-01],\n",
      "         [-2.6868e-03,  4.2130e-01,  6.8803e-01,  ...,  4.6363e-01,\n",
      "           6.4152e-01,  3.0779e-01],\n",
      "         [-2.6868e-03,  4.2130e-01,  6.8803e-01,  ...,  4.6363e-01,\n",
      "           6.4152e-01,  3.0779e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.2517e-01, -1.2226e+00, -5.2424e-01,  ..., -6.5567e-01,\n",
      "           5.7127e-02, -9.3601e-01],\n",
      "         [-1.2517e-01, -1.2226e+00, -5.2424e-01,  ..., -6.5567e-01,\n",
      "           5.7127e-02, -9.3601e-01],\n",
      "         [-1.2517e-01, -1.2226e+00, -5.2424e-01,  ..., -6.5567e-01,\n",
      "           5.7127e-02, -9.3601e-01],\n",
      "         ...,\n",
      "         [-1.2517e-01, -1.2226e+00, -5.2424e-01,  ..., -6.5567e-01,\n",
      "           5.7127e-02, -9.3601e-01],\n",
      "         [-1.2517e-01, -1.2226e+00, -5.2424e-01,  ..., -6.5567e-01,\n",
      "           5.7127e-02, -9.3601e-01],\n",
      "         [-1.2517e-01, -1.2226e+00, -5.2424e-01,  ..., -6.5567e-01,\n",
      "           5.7127e-02, -9.3601e-01]],\n",
      "\n",
      "        [[-5.0814e-01,  1.6430e+00,  7.9782e-01,  ..., -8.9397e-01,\n",
      "           4.4636e-01,  2.2225e-04],\n",
      "         [-5.0814e-01,  1.6430e+00,  7.9782e-01,  ..., -8.9397e-01,\n",
      "           4.4636e-01,  2.2225e-04],\n",
      "         [-5.0814e-01,  1.6430e+00,  7.9782e-01,  ..., -8.9397e-01,\n",
      "           4.4636e-01,  2.2225e-04],\n",
      "         ...,\n",
      "         [-5.0814e-01,  1.6430e+00,  7.9782e-01,  ..., -8.9397e-01,\n",
      "           4.4636e-01,  2.2225e-04],\n",
      "         [-5.0814e-01,  1.6430e+00,  7.9782e-01,  ..., -8.9397e-01,\n",
      "           4.4636e-01,  2.2225e-04],\n",
      "         [-5.0814e-01,  1.6430e+00,  7.9782e-01,  ..., -8.9397e-01,\n",
      "           4.4636e-01,  2.2225e-04]],\n",
      "\n",
      "        [[-3.1768e-01, -1.5493e-01,  2.7377e-01,  ...,  5.7172e-01,\n",
      "           1.4009e-01,  1.0499e-01],\n",
      "         [-3.1768e-01, -1.5493e-01,  2.7377e-01,  ...,  5.7172e-01,\n",
      "           1.4009e-01,  1.0499e-01],\n",
      "         [-3.1768e-01, -1.5493e-01,  2.7377e-01,  ...,  5.7172e-01,\n",
      "           1.4009e-01,  1.0499e-01],\n",
      "         ...,\n",
      "         [-3.1768e-01, -1.5493e-01,  2.7377e-01,  ...,  5.7172e-01,\n",
      "           1.4009e-01,  1.0499e-01],\n",
      "         [-3.1768e-01, -1.5493e-01,  2.7377e-01,  ...,  5.7172e-01,\n",
      "           1.4009e-01,  1.0499e-01],\n",
      "         [-3.1768e-01, -1.5493e-01,  2.7377e-01,  ...,  5.7172e-01,\n",
      "           1.4009e-01,  1.0499e-01]]], grad_fn=<ExpandBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SensorEmbeddingLayer(nn.Module):\n",
    "    \"\"\"Embedding layer for sensor features.\"\"\"\n",
    "\n",
    "    def __init__(self, num_sensor: int, time_length: int, dim_embedding: int):\n",
    "        super(SensorEmbeddingLayer, self).__init__()\n",
    "        self.num_sensor = num_sensor\n",
    "        self.embedding_size = time_length\n",
    "        self.dim_embedding = dim_embedding\n",
    "\n",
    "        # Define the embedding layer\n",
    "        self.sensor_embedding = nn.Linear(time_length, dim_embedding)\n",
    "\n",
    "    def forward(self, sensor_matrix: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply sensor embedding to the input sensor matrix.\n",
    "        \n",
    "        Parameters:\n",
    "        sensor_matrix (torch.Tensor): Input tensor of shape (num_sensor, embedding_size)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor of shape (num_sensor, embedding_size, dim_embedding)\n",
    "        \"\"\"\n",
    "        # Apply the embedding layer to each sensor in the matrix\n",
    "        embedded_matrix = self.sensor_embedding(sensor_matrix)\n",
    "        \n",
    "        # Unsqueeze to add the time dimension\n",
    "        embedded_matrix = embedded_matrix.unsqueeze(1)  # Shape: (num_sensor, 1, dim_embedding)\n",
    "\n",
    "        # Expand along the time dimension\n",
    "        embedded_matrix = embedded_matrix.expand(-1, self.embedding_size, -1)  # Shape: (num_sensor, time_length, dim_embedding)\n",
    "\n",
    "        return embedded_matrix\n",
    "        \n",
    "        return embedded_matrix\n",
    "\n",
    "# Example usage\n",
    "num_sensor = 37\n",
    "embedding_size = length_time\n",
    "dim_embedding = 32\n",
    "\n",
    "# Create the sensor embedding layer\n",
    "sensor_embedding_layer = SensorEmbeddingLayer(num_sensor, embedding_size, dim_embedding)\n",
    "\n",
    "\n",
    "# Get the embedded output\n",
    "sensor_embedded_output = sensor_embedding_layer(data_1)\n",
    "\n",
    "print(\"Embedded Output Shape:\", sensor_embedded_output.shape)\n",
    "print(\"Embedded Output:\", sensor_embedded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIME EMBEDDING LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time Embedded Output Shape: torch.Size([1, 50, 32])\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import Any, Optional\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BigBirdConfig, MambaConfig\n",
    "\n",
    "class TimeEmbeddingLayer(nn.Module):\n",
    "    \"\"\"Embedding layer for time features.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size: int, is_time_delta: bool = False):\n",
    "        super().__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.is_time_delta = is_time_delta\n",
    "\n",
    "        self.w = nn.Parameter(torch.empty(1, self.embedding_size))\n",
    "        self.phi = nn.Parameter(torch.empty(1, self.embedding_size))\n",
    "\n",
    "        nn.init.xavier_uniform_(self.w)\n",
    "        nn.init.xavier_uniform_(self.phi)\n",
    "\n",
    "    def forward(self, time_stamps: torch.Tensor) -> Any:\n",
    "        \"\"\"Apply time embedding to the input time stamps.\"\"\"\n",
    "        if self.is_time_delta:\n",
    "            # If the time_stamps represent time deltas, we calculate the deltas.\n",
    "            # This is equivalent to the difference between consecutive elements.\n",
    "            time_stamps = torch.cat(\n",
    "                (time_stamps[:, 0:1] * 0, time_stamps[:, 1:] - time_stamps[:, :-1]),\n",
    "                dim=-1,\n",
    "            )\n",
    "        time_stamps = time_stamps.float()\n",
    "        time_stamps_expanded = time_stamps.unsqueeze(-1)\n",
    "        next_input = time_stamps_expanded * self.w + self.phi\n",
    "\n",
    "        return torch.sin(next_input)\n",
    "    \n",
    "# Example usage\n",
    "\n",
    "time_layer = TimeEmbeddingLayer(dim_embedding, is_time_delta=False)\n",
    "\n",
    "# Get the embedded output\n",
    "time_embedded_output = time_layer(time)\n",
    "\n",
    "print(\"Time Embedded Output Shape:\", time_embedded_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STATIC EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static Embedded Output Shape: torch.Size([32])\n",
      "Static Embedded Output: tensor([ 0.7477,  0.4308,  0.0703, -0.4131, -0.2980,  0.5652, -0.7332,  0.4662,\n",
      "        -0.5202,  0.1866,  1.1226,  0.2759,  1.1025, -0.5448, -0.7484,  1.3939,\n",
      "         0.1168,  0.7116,  1.4629, -0.7686, -0.2416, -0.3767,  1.1720,  0.5435,\n",
      "         0.5205, -1.6172,  0.4486, -0.5138, -0.4031, -0.5404, -0.3838,  1.2161],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class StaticEmbeddings(nn.Module):\n",
    "    \"\"\"Embedding layer for static features.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim: int, embedding_dim: int):\n",
    "        super(StaticEmbeddings, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Define the embedding layer\n",
    "        self.embedding_layer = nn.Linear(input_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, static_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply embedding to the input static features.\n",
    "        \n",
    "        Parameters:\n",
    "        static_features (torch.Tensor): Input tensor of shape (batch_size, input_dim)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Output tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Apply the embedding layer\n",
    "        embedded_features = self.embedding_layer(static_features)\n",
    "        \n",
    "        return embedded_features\n",
    "\n",
    "# Example usage\n",
    "input_dim = static.shape[1]\n",
    "embedding_dim = 32  # Ensure this matches the desired embedding dimension\n",
    "\n",
    "# Create the static embedding layer\n",
    "static_embedding_layer = StaticEmbeddings(input_dim, embedding_dim)\n",
    "\n",
    "# Get the embedded output\n",
    "static_embedded_output = static_embedding_layer(static_1)\n",
    "\n",
    "print(\"Static Embedded Output Shape:\", static_embedded_output.shape)\n",
    "print(\"Static Embedded Output:\", static_embedded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TOTAL EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Embedded Output Shape: torch.Size([37, 50, 32])\n",
      "Combined Embedded Output: tensor([[[-1.9350e+00,  7.2320e-02, -3.9955e-01,  ...,  1.2593e+00,\n",
      "           7.2569e-01,  2.6458e-01],\n",
      "         [-5.2256e-01,  1.5626e-01, -1.4258e+00,  ...,  4.7073e-01,\n",
      "           3.8697e-02, -1.5231e+00],\n",
      "         [-1.9350e+00,  7.2320e-02, -3.9955e-01,  ...,  1.2593e+00,\n",
      "           7.2569e-01,  2.6458e-01],\n",
      "         ...,\n",
      "         [-1.7623e+00,  1.1784e-01, -1.3839e+00,  ..., -9.0421e-02,\n",
      "           1.0251e-01, -1.0962e+00],\n",
      "         [-5.0239e-01, -1.6156e+00, -2.8773e-01,  ...,  1.6144e-01,\n",
      "          -1.1364e+00, -1.8484e-01],\n",
      "         [-1.4410e+00, -1.8103e+00, -1.2573e+00,  ...,  1.3058e+00,\n",
      "           2.9971e-01,  2.8766e-01]],\n",
      "\n",
      "        [[-1.8173e+00,  1.0718e+00, -5.9501e-01,  ...,  1.1334e+00,\n",
      "           2.3187e+00, -6.2941e-01],\n",
      "         [-4.0481e-01,  1.1557e+00, -1.6213e+00,  ...,  3.4479e-01,\n",
      "           1.6317e+00, -2.4171e+00],\n",
      "         [-1.8173e+00,  1.0718e+00, -5.9501e-01,  ...,  1.1334e+00,\n",
      "           2.3187e+00, -6.2941e-01],\n",
      "         ...,\n",
      "         [-1.6445e+00,  1.1173e+00, -1.5794e+00,  ..., -2.1636e-01,\n",
      "           1.6955e+00, -1.9902e+00],\n",
      "         [-3.8464e-01, -6.1620e-01, -4.8319e-01,  ...,  3.5499e-02,\n",
      "           4.5664e-01, -1.0788e+00],\n",
      "         [-1.3232e+00, -8.1086e-01, -1.4527e+00,  ...,  1.1799e+00,\n",
      "           1.8927e+00, -6.0632e-01]],\n",
      "\n",
      "        [[-3.2760e+00,  3.4192e-01, -4.1876e-01,  ...,  6.7761e-01,\n",
      "           6.8528e-01, -6.2503e-01],\n",
      "         [-1.8636e+00,  4.2585e-01, -1.4450e+00,  ..., -1.1099e-01,\n",
      "          -1.7162e-03, -2.4127e+00],\n",
      "         [-3.2760e+00,  3.4192e-01, -4.1876e-01,  ...,  6.7761e-01,\n",
      "           6.8528e-01, -6.2503e-01],\n",
      "         ...,\n",
      "         [-3.1033e+00,  3.8743e-01, -1.4031e+00,  ..., -6.7214e-01,\n",
      "           6.2095e-02, -1.9858e+00],\n",
      "         [-1.8434e+00, -1.3460e+00, -3.0693e-01,  ..., -4.2028e-01,\n",
      "          -1.1768e+00, -1.0745e+00],\n",
      "         [-2.7820e+00, -1.5407e+00, -1.2765e+00,  ...,  7.2412e-01,\n",
      "           2.5929e-01, -6.0195e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.7019e+00,  4.8776e-01, -1.5482e+00,  ...,  2.0965e+00,\n",
      "           1.4402e+00,  1.2691e+00],\n",
      "         [-2.8944e-01,  5.7170e-01, -2.5744e+00,  ...,  1.3079e+00,\n",
      "           7.5321e-01, -5.1860e-01],\n",
      "         [-1.7019e+00,  4.8776e-01, -1.5482e+00,  ...,  2.0965e+00,\n",
      "           1.4402e+00,  1.2691e+00],\n",
      "         ...,\n",
      "         [-1.5291e+00,  5.3328e-01, -2.5325e+00,  ...,  7.4674e-01,\n",
      "           8.1702e-01, -9.1676e-02],\n",
      "         [-2.6926e-01, -1.2002e+00, -1.4363e+00,  ...,  9.9860e-01,\n",
      "          -4.2185e-01,  8.1966e-01],\n",
      "         [-1.2079e+00, -1.3949e+00, -2.4059e+00,  ...,  2.1430e+00,\n",
      "           1.0142e+00,  1.2922e+00]],\n",
      "\n",
      "        [[-2.3535e+00,  2.0460e+00, -1.6521e-01,  ...,  9.9685e-01,\n",
      "           3.5782e-01,  2.3045e-01],\n",
      "         [-9.4105e-01,  2.1300e+00, -1.1915e+00,  ...,  2.0824e-01,\n",
      "          -3.2917e-01, -1.5572e+00],\n",
      "         [-2.3535e+00,  2.0460e+00, -1.6521e-01,  ...,  9.9685e-01,\n",
      "           3.5782e-01,  2.3045e-01],\n",
      "         ...,\n",
      "         [-2.1808e+00,  2.0915e+00, -1.1496e+00,  ..., -3.5291e-01,\n",
      "          -2.6536e-01, -1.1303e+00],\n",
      "         [-9.2088e-01,  3.5806e-01, -5.3389e-02,  ..., -1.0105e-01,\n",
      "          -1.5042e+00, -2.1897e-01],\n",
      "         [-1.8595e+00,  1.6340e-01, -1.0229e+00,  ...,  1.0433e+00,\n",
      "          -6.8164e-02,  2.5353e-01]],\n",
      "\n",
      "        [[-1.7702e+00,  4.3064e-01, -8.2843e-01,  ...,  7.4451e-01,\n",
      "           9.8185e-01, -8.6975e-01],\n",
      "         [-3.5771e-01,  5.1457e-01, -1.8547e+00,  ..., -4.4090e-02,\n",
      "           2.9486e-01, -2.6574e+00],\n",
      "         [-1.7702e+00,  4.3064e-01, -8.2843e-01,  ...,  7.4451e-01,\n",
      "           9.8185e-01, -8.6975e-01],\n",
      "         ...,\n",
      "         [-1.5974e+00,  4.7615e-01, -1.8128e+00,  ..., -6.0524e-01,\n",
      "           3.5867e-01, -2.2305e+00],\n",
      "         [-3.3753e-01, -1.2573e+00, -7.1661e-01,  ..., -3.5338e-01,\n",
      "          -8.8020e-01, -1.3192e+00],\n",
      "         [-1.2761e+00, -1.4520e+00, -1.6861e+00,  ...,  7.9102e-01,\n",
      "           5.5587e-01, -8.4667e-01]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CombinedEmbeddings(nn.Module):\n",
    "    \"\"\"Combined embedding layer for sensor, time, and static features.\"\"\"\n",
    "\n",
    "    def __init__(self, num_sensor: int, time_lenght: int, dim_embedding: int, static_input_dim: int, is_time_delta: bool = False):\n",
    "        super(CombinedEmbeddings, self).__init__()\n",
    "        \n",
    "        # Initialize sensor embedding layer\n",
    "        self.sensor_embedding_layer = SensorEmbeddingLayer(num_sensor, time_lenght, dim_embedding)\n",
    "        \n",
    "        # Initialize time embedding layer\n",
    "        self.time_embedding_layer = TimeEmbeddingLayer(dim_embedding, is_time_delta)\n",
    "        \n",
    "        # Initialize static embedding layer\n",
    "        self.static_embedding_layer = StaticEmbeddings(static_input_dim, dim_embedding)\n",
    "\n",
    "    def forward(self, sensor_matrix: torch.Tensor, time_stamps: torch.Tensor, static_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply combined embeddings to the input sensor matrix, time stamps, and static features.\n",
    "        \n",
    "        Parameters:\n",
    "        sensor_matrix (torch.Tensor): Input tensor of shape (num_sensor, sensor_embedding_size)\n",
    "        time_stamps (torch.Tensor): Input tensor of shape (1, time_embedding_size)\n",
    "        static_features (torch.Tensor): Input tensor of shape (batch_size, static_input_dim)\n",
    "        \n",
    "        Returns:\n",
    "        torch.Tensor: Concatenated output tensor of shape (batch_size, combined_embedding_dim)\n",
    "        \"\"\"\n",
    "        # Apply sensor embedding\n",
    "        sensor_embedded = self.sensor_embedding_layer(sensor_matrix) # output shape (num_sensor, time_lenght, dim_embedding)\n",
    "        \n",
    "        # Apply time embedding\n",
    "        time_embedded = self.time_embedding_layer(time_stamps) # output shape (1, time_lenght, dim_embedding)\n",
    "        print(\"Before expand\",time_embedded.shape)\n",
    "        time_embedded = time_embedded.expand(sensor_embedded.shape) # transform to shape (num_sensor, time_lenght, dim_embedding)\n",
    "        print(\"After expand\",time_embedded.shape)\n",
    "        \n",
    "        # Apply static embedding\n",
    "        static_embedded = self.static_embedding_layer(static_features) # output shape (dim_embedding)\n",
    "        static_embedded = static_embedded.unsqueeze(0).unsqueeze(0).expand(sensor_embedded.shape) # transform to shape (num_sensor, time_lenght, dim_embedding)\n",
    "        \n",
    "        # Combine all embeddings by adding them together\n",
    "        combined_embedding = sensor_embedded + time_embedded + static_embedded\n",
    "        \n",
    "        return combined_embedding\n",
    "\n",
    "# Example usage\n",
    "combined_embedding_layer = CombinedEmbeddings(num_sensor, embedding_size, dim_embedding, input_dim, is_time_delta=False) \n",
    "\n",
    "# Get the combined embedded output\n",
    "combined_embedded_output = combined_embedding_layer(data_1, time, static_1)\n",
    "\n",
    "print(\"Combined Embedded Output Shape:\", combined_embedded_output.shape)\n",
    "print(\"Combined Embedded Output:\", combined_embedded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAMBA MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed datasets from ./processed_datasets\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_pos.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_neg.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_val.h5\n",
      "Loaded dataset from ./processed_datasets/physionet2012_1_test.h5\n"
     ]
    }
   ],
   "source": [
    "from mortality_part_preprocessing import MortalityDataset, PairedDataset, load_pad_separate\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "\n",
    "\n",
    "train_batch_size = batch_size // 2  # we concatenate 2 batches together\n",
    "\n",
    "train_collate_fn = PairedDataset.paired_collate_fn_truncate\n",
    "val_test_collate_fn = MortalityDataset.non_pair_collate_fn_truncate\n",
    "\n",
    "base_path = './P12data'\n",
    "\n",
    "base_path_new = f\"{base_path}/split_{1}\"\n",
    "\n",
    "\n",
    "train_pair, val_data, test_data = load_pad_separate(\n",
    "    'physionet2012', base_path_new, 1\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_pair, train_batch_size, shuffle=True, num_workers=16, collate_fn=train_collate_fn, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size, shuffle=True, num_workers=16, collate_fn=val_test_collate_fn, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size, shuffle=False, num_workers=16, collate_fn=val_test_collate_fn, pin_memory=True)\n",
    "\n",
    "iterable_inner_dataloader = iter(train_dataloader) # make the train_dataloader iterable\n",
    "test_batch = next(iterable_inner_dataloader) # iterate on the next object in a tuple\n",
    "max_seq_length = test_batch[0].shape[2] # shape[2] = T\n",
    "sensor_count = test_batch[0].shape[1] # shape[1] = F\n",
    "static_size = test_batch[2].shape[1] # shape[1] = 8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MambaFoo(nn.Module):\n",
    "    def __init__(self, num_sensor: int, time_lenght: int, dim_embedding: int, static_input_dim: int, is_time_delta: bool = False):\n",
    "        super(MambaFoo, self).__init__()\n",
    "        \n",
    "        self.embedding = CombinedEmbeddings(\n",
    "            num_sensor=num_sensor,\n",
    "            time_lenght=time_lenght,\n",
    "            dim_embedding=dim_embedding,\n",
    "            static_input_dim=static_input_dim,\n",
    "            is_time_delta=is_time_delta\n",
    "        )\n",
    "\n",
    "    def forward(self, sensor_matrix: torch.Tensor, time_stamps: torch.Tensor, static_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for MambaFoo.\n",
    "        \n",
    "        Parameters:\n",
    "        - sensor_matrix: Input sensor data (batch_size, num_sensor, time_lenght)\n",
    "        - time_stamps: Input time data (1, time_lenght)\n",
    "        - static_features: Input static features (batch_size, static_input_dim)\n",
    "        \n",
    "        Returns:\n",
    "        - torch.Tensor: Output embedding of shape (batch_size, num_sensor, time_lenght, dim_embedding)\n",
    "        \"\"\"\n",
    "        # Compute combined embedding\n",
    "        combined_embedding = self.embedding(sensor_matrix, time_stamps, static_features)\n",
    "        \n",
    "        # Optionally, add additional layers or computations here\n",
    "        \n",
    "        return combined_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2013 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2013 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 37, 94])\n",
      "entering into patient 0/4\n",
      "torch.Size([37, 94])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (94) must match the existing size (50) at non-singleton dimension 1.  Target sizes: [37, 94, 32].  Tensor sizes: [50, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 25\u001b[0m\n\u001b[1;32m     16\u001b[0m mambafoo \u001b[38;5;241m=\u001b[39m MambaFoo(\n\u001b[1;32m     17\u001b[0m num_sensor\u001b[38;5;241m=\u001b[39mpatient\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \n\u001b[1;32m     18\u001b[0m time_lenght\u001b[38;5;241m=\u001b[39mpatient\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m is_time_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass with sample inputs\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmambafoo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_patient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_patient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMambaFoo Output Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMambaFoo Output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[0;32m~/deep-learning-02456/EHR_Mamba_model_89/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-learning-02456/EHR_Mamba_model_89/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[115], line 26\u001b[0m, in \u001b[0;36mMambaFoo.forward\u001b[0;34m(self, sensor_matrix, time_stamps, static_features)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03mForward pass for MambaFoo.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m- torch.Tensor: Output embedding of shape (batch_size, num_sensor, time_lenght, dim_embedding)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Compute combined embedding\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m combined_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensor_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_stamps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Optionally, add additional layers or computations here\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_embedding\n",
      "File \u001b[0;32m~/deep-learning-02456/EHR_Mamba_model_89/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/deep-learning-02456/EHR_Mamba_model_89/venv/lib64/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[113], line 33\u001b[0m, in \u001b[0;36mCombinedEmbeddings.forward\u001b[0;34m(self, sensor_matrix, time_stamps, static_features)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Apply time embedding\u001b[39;00m\n\u001b[1;32m     32\u001b[0m time_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding_layer(time_stamps) \u001b[38;5;66;03m# output shape (1, time_lenght, dim_embedding)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m time_embedded \u001b[38;5;241m=\u001b[39m \u001b[43mtime_embedded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensor_embedded\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# transform to shape (num_sensor, time_lenght, dim_embedding)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Apply static embedding\u001b[39;00m\n\u001b[1;32m     36\u001b[0m static_embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatic_embedding_layer(static_features) \u001b[38;5;66;03m# output shape (dim_embedding)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (94) must match the existing size (50) at non-singleton dimension 1.  Target sizes: [37, 94, 32].  Tensor sizes: [50, 32]"
     ]
    }
   ],
   "source": [
    "for batch in tqdm.tqdm(train_dataloader, total=len(train_dataloader)):\n",
    "    data, times, static, labels, mask, delta = batch\n",
    "\n",
    "    print(data.shape)\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "\n",
    "        print(f\"entering into patient {i}/{data.shape[0]}\")\n",
    "        \n",
    "        patient = data[i,:,:]\n",
    "        time_patient = time[i,:]\n",
    "        static_patient = static[i,:]\n",
    "\n",
    "        print(patient.shape)\n",
    "\n",
    "        mambafoo = MambaFoo(\n",
    "        num_sensor=patient.shape[0], \n",
    "        time_lenght=patient.shape[1], \n",
    "        dim_embedding=32, \n",
    "        static_input_dim=8, \n",
    "        is_time_delta=False\n",
    "        )\n",
    "        \n",
    "        # Forward pass with sample inputs\n",
    "        output = mambafoo(patient, time_patient, static_patient)\n",
    "\n",
    "        print(\"MambaFoo Output Shape:\", output.shape)\n",
    "        print(\"MambaFoo Output:\", output)\n",
    "\n",
    "    \"Continue this:\"\n",
    "    break\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
